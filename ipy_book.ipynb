{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re,unirest\n",
    "import treq\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn=psycopg2.connect(database=\"resultdb\", user=\"postgres\",password=\"wszgrwhja1\", host=\"10.1.36.183\", port=\"5432\")\n",
    "cur = conn.cursor()\n",
    "header={'host':'www.onetonline.org',\n",
    "        'User-Agent':'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:43.0) Gecko/20100101 Firefox/43.0'}\n",
    "\n",
    "#sql="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##related_code_content\n",
    "conn.commit()\n",
    "sql1='''CREATE TABLE related_code_content_test \n",
    "       (id serial not null PRIMARY KEY,\n",
    "       related_url varchar(90) not null   ,\n",
    "       field  varchar(20)  null,\n",
    "       code  varchar(10)  null,\n",
    "       related_content  character varying  null\n",
    ")'''\n",
    "##related_code\n",
    "##error_url\n",
    "sql='''CREATE TABLE error_url_test\n",
    "       (id serial not null PRIMARY KEY,\n",
    "       related_url varchar(90) not null   ,\n",
    "       type varchar(50) not null   ,\n",
    "       time  timestamp  not null default current_timestamp\n",
    ")'''\n",
    "sql='''CREATE TABLE related_code \n",
    "       (id serial not null PRIMARY KEY,\n",
    "       related_url varchar(90) not null   ,\n",
    "       field  varchar(50)  null,\n",
    "       code  varchar(10)   null\n",
    ")'''\n",
    "\n",
    "cur.execute(sql)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "all_type=[\"detailed_work_activities\",\"work_context\",   \"work_values_content\",  \n",
    "\"work_styles_content\", \"work_activities\",  \"skills_content\",  \"knowledge_content\", \"interests\",  \"abilities\"]  \n",
    "types='abilities'\n",
    "#for each in all_type:\n",
    "sql='select related_url from %s;'% types\n",
    "cur.execute(sql)\n",
    "row=cur.fetchall()\n",
    "conn.commit()\n",
    "#get_more_touch(row,each)\n",
    "temp_url=row[1:6]\n",
    "\n",
    "get_more_touchs(temp_url,types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##抓取相关的数据 并存到数据库中  \n",
    "\n",
    "##获取进一步的链接 并返回列表 列表是一个字典 （带相关内容的）\n",
    "def get_more_touchs(list_content,types):\n",
    "    for each in list_content:\n",
    "        url=each[0]\n",
    "        try:\n",
    "            r=requests.get(url,headers=header)\n",
    "            soup=BeautifulSoup(r.text)\n",
    "            websites={\n",
    "            \"detailed_work_activities\" :\"/search/dwa/compare/.*?g=Continue\",\n",
    "               \"work_context\":\"^/find/descriptor/result/.*?\",\n",
    "               \"work_values_content\":\"^/explore/workvalues/.*?\",\n",
    "               \"work_styles_content\":\"^/find/descriptor/result/.*?\",\n",
    "               \"work_activities\":\"^/find/descriptor/result/.*?\",\n",
    "               \"skills_content\":\"^/find/descriptor/result/.*?\",\n",
    "                \"knowledge_content\":\"^/find/descriptor/result/.*?\",\n",
    "                \"interests\":\"^/explore/interests/.*?\",\n",
    "                  \"abilities\":\"^/explore/interests/.*?\"\n",
    "            }\n",
    "            re_type=websites[types]\n",
    "            yes=soup.find('a',href=re.compile(re_type))\n",
    "            if yes:\n",
    "                more_url='http://www.onetonline.org'+yes['href']\n",
    "                r=requests.get(more_url,headers=header)\n",
    "                soup1=BeautifulSoup(r.text)\n",
    "                if types!='detailed_work_activities':\n",
    "                    content=get_more_code(soup1)\n",
    "                else:\n",
    "                    content=[each.get_text() for each in soup1.find_all('td',class_='occcode')]\n",
    "                for code in content:\n",
    "                        cur.execute('insert into related_code(related_url,field,code) values(%s,%s,%s)',[url,types,code])\n",
    "                        conn.commit()\n",
    "            else:\n",
    "                codes=[each.get_text() for each in soup.find_all('td',class_='occcode')]\n",
    "                if codes:\n",
    "                    for code in codes:\n",
    "                        cur.execute('insert into related_code(related_url,field,code) values(%s,%s,%s)',[url,types,code])\n",
    "                        conn.commit()\n",
    "                else:\n",
    "                        cur.execute('insert into related_code(related_url,field) values(%s,%s)',[url,types])\n",
    "                        conn.commit()\n",
    "        except Exception as e:\n",
    "            conn.commit()\n",
    "            print each,e\n",
    "            sql='select related_url from error_url_test where related_url = \\'%s\\' ' % each\n",
    "            cur.execute(sql)\n",
    "            a=cur.fetchone()\n",
    "            if not a:\n",
    "                print url\n",
    "                cur.execute('insert into error_url_test(related_url,type) values(%s,%s)' ,[url,types])\n",
    "                conn.commit()\n",
    "            continue\n",
    "    \n",
    "def get_more_code(soup):\n",
    "    #返回一个列表r eportrt\n",
    "    codes=[each.get_text() for each in soup.find_all('td',class_='reportrt')]\n",
    "    return codes\n",
    "\n",
    "def get_more_touch(list_content,types):\n",
    "    for each in list_content:\n",
    "        url=each[0]\n",
    "        try:\n",
    "            r=requests.get(url,headers=header)\n",
    "            soup=BeautifulSoup(r.text)\n",
    "            a=soup.find('a',href=re.compile('^/search/t2/occs/\\d{4,15}'))\n",
    "            if a:\n",
    "                more_url='http://www.onetonline.org'+a['href']\n",
    "                r=requests.get(more_url,headers=header)\n",
    "                soup1=BeautifulSoup(r.text)\n",
    "                #time.sleep(1)\n",
    "                content=get_more_codes(soup1)\n",
    "                for one in content:\n",
    "                    code=one[0]\n",
    "                    task=one[1]\n",
    "                    cur.execute('insert into related_code_content_test(related_url,field,code,related_content) values(%s,%s,%s,%s)',[each,types,code,task])\n",
    "                    conn.commit()\n",
    "            else:\n",
    "                content=get_more_codes(soup1)\n",
    "                if content:\n",
    "                    for one in content:\n",
    "                        code=one[0]\n",
    "                        task=one[1]\n",
    "                        cur.execute('insert into related_code_content_test(related_url,field,code,related_content) values(%s,%s,%s,%s)',[each,types,code,task])\n",
    "                        conn.commit()\n",
    "                else:\n",
    "                        cur.execute('insert into related_code_content_test(related_url,field) values(%s,%s)',[each,types])\n",
    "                        conn.commit()\n",
    "        except Exception as e:\n",
    "            conn.commit()\n",
    "            print each,e\n",
    "            sql='select related_url from error_url_test where related_url = \\'%s\\' ' % each\n",
    "            cur.execute(sql)\n",
    "            a=cur.fetchone()\n",
    "            if not a:\n",
    "                cur.execute('insert into error_url_test(related_url,type) values(%s,%s)' ,[each,types])\n",
    "                conn.commit()\n",
    "            continue\n",
    "                \n",
    "    \n",
    "def get_more_codes(soup):\n",
    "    codes=[each.get_text() for each in soup.find_all('td',class_='reportrtd')]\n",
    "    content=[each.get_text() for each in soup.select('td.report2ed div')]\n",
    "    return zip(codes,content)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##针对task的\n",
    "def get_more(list_urls):\n",
    "    try:\n",
    "            for url in list_urls:\n",
    "                each=url[0]\n",
    "                try:\n",
    "                    r=requests.get(each,headers=header)\n",
    "                    soup=BeautifulSoup(r.text)\n",
    "                    yes=soup.find('a',href=re.compile('^/search/task/compare/.*?'))\n",
    "                    if yes:\n",
    "                        more_url='http://www.onetonline.org'+yes['href']\n",
    "                        #time.sleep(1)\n",
    "                        r=requests.get(each,headers=header)\n",
    "                        soup1=BeautifulSoup(r.text)\n",
    "                        content=get_each(soup1)\n",
    "                        for one in content:\n",
    "                            code=one[0]\n",
    "                            task=one[1]\n",
    "                            for two in task:\n",
    "                                cur.execute('insert into related_code_content(related_url,field,code,related_content) values(%s,\\'task\\',%s,%s)',[each,code,two])\n",
    "                                conn.commit()\n",
    "                    else:\n",
    "                        content=get_each(soup)\n",
    "                        if content:\n",
    "                            for one in content:\n",
    "                                code=one[0]\n",
    "                                task=one[1]\n",
    "                                for two in task:\n",
    "                                    cur.execute('insert into related_code_content(related_url,field,code,related_content) values(%s,\\'task\\',%s,%s)',[each,code,two])\n",
    "                                conn.commit()\n",
    "\n",
    "                        else:\n",
    "                            cur.execute('insert into related_code_content(related_url,field) values(%s,\\'task\\')',[each])\n",
    "                            conn.commit()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print each,e\n",
    "                    sql='select related_url from error_url where related_url = \\'%s\\' ' % each\n",
    "                    cur.execute(sql)\n",
    "                    a=cur.fetchone()\n",
    "                    if not a:\n",
    "                        cur.execute('insert into error_url(related_url,type) values(%s,\\'task\\')' ,[each])\n",
    "                        conn.commit()\n",
    "                    continue\n",
    "    except Exception as e :\n",
    "            print e\n",
    "            \n",
    "    ##  带相关内容的 需要点击  返回字典格式\n",
    "def get_each(soup):\n",
    "    codes=[each.get_text() for each in soup.find_all('td',class_='occcode')]\n",
    "    #all_content=[each.get_text() for each in soup.find_all('a',href=re.compile('^http://www.onetonline.org/link/summary/\\d.*?'))]\n",
    "    #nums= [ int(each.get_text().strip(' closely related')) for each in soup.find_all('a',class_='toggle')]\n",
    "    all_tasks=[each.get_text().strip('\\n') for each in soup.find_all('ul',class_='toggle')]\n",
    "    temp=[]\n",
    "    i=0\n",
    "    for each in all_tasks:\n",
    "        temp1=each.split('\\n')\n",
    "        temp.append(temp1)\n",
    "    return zip(codes,temp)\n",
    "\n",
    "\n",
    "\n",
    "    ##找到链接 然后点击更多\n",
    "    \n",
    "\n",
    "        \n",
    "    ##找到链接 然后点击更多\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "conn=psycopg2.connect(database=\"resultdb\", user=\"postgres\",password=\"wszgrwhja1\", host=\"10.1.36.183\", port=\"5432\")\n",
    "cur = conn.cursor()\n",
    "from pyspider.database import connect_database\n",
    "resultdb = connect_database(\"sqlalchemy+postgresql+resultdb://postgres:wszgrwhja1@10.1.36.183:5432/resultdb\")\n",
    "#result=resultdb.select('test6').next()\n",
    "#row_result = result['result']\n",
    "#url=row_result['wages_and_employment_content']\n",
    "#print type(url),url\n",
    "#\n",
    "conn.commit()\n",
    "for result in  resultdb.select('test6'):\n",
    "# major_title\n",
    "    try:\n",
    "        row_result = result['result']\n",
    "        url=row_result['current_url']\n",
    "        sql='select url_id from major_table where url = \\'%s\\' ' % url\n",
    "        cur.execute(sql)\n",
    "        a=cur.fetchone()\n",
    "        if a:\n",
    "            url_id=a[0]\n",
    "            wages_and_employment_content =row_result['wages_and_employment_content']##这里和下面得改\n",
    "            if  wages_and_employment_content :##这里得该\n",
    "                median_wages=wages_and_employment_content[0]#这里得修改\n",
    "                median_wages[0]=median_wages[0].encode('utf-8')\n",
    "                if '(' in median_wages[0]:\n",
    "                    median_wages[0]=median_wages[0].split('(')[1].strip(' )')\n",
    "                median_wages=':'.join(median_wages)\n",
    "                employment=wages_and_employment_content[1]#这里得修改\n",
    "                employment[0]=employment[0].encode('utf-8')\n",
    "                if '(' in employment[0]:\n",
    "                    employment[0]=employment[0].split('(')[1].strip(' )')\n",
    "                employment=':'.join(employment)\n",
    "                projected_growth=wages_and_employment_content[2]#这里得修改\n",
    "                projected_growth[0]=projected_growth[0].encode('utf-8')\n",
    "                if '(' in projected_growth[0]:\n",
    "                    projected_growth[0]=projected_growth[0].split('(')[1].strip(' )')\n",
    "                projected_growth=':'.join(projected_growth)\n",
    "                projected_job_openings=wages_and_employment_content[3]#这里得修改\n",
    "                projected_job_openings[0]=projected_job_openings[0].encode('utf-8')\n",
    "                if '(' in projected_job_openings[0]:\n",
    "                    projected_job_openings[0]=projected_job_openings[0].split('(')[1].strip(' )')\n",
    "                projected_job_openings=':'.join(projected_job_openings)\n",
    "                top_industries=wages_and_employment_content[4]#这里得修改\n",
    "                top_industries[0]=top_industries[0].encode('utf-8')\n",
    "                if '(' in top_industries[0]:\n",
    "                    top_industries[0]=top_industries[0].split('(')[1].strip(' )')                \n",
    "                top_industries=':'.join(top_industries)\n",
    "                cur.execute( \"INSERT INTO wages_and_employment_content (url_id,median_wages,employment,projected_growth,projected_job_openings,top_industries) VALUES (%s,%s,%s,%s,%s,%s)\",\n",
    "                  [url_id, median_wages,employment,projected_growth,projected_job_openings,top_industries]) ##这里得改\n",
    "                conn.commit()\n",
    "            else:\n",
    "                cur.execute( \"INSERT INTO wages_and_employment_content (url_id,median_wages,employment,projected_growth,projected_job_openings,top_industries) VALUES (%s,%s,%s,%s,%s,%s)\",\n",
    "                      [url_id,'','','','',''])  ##这里得改\n",
    "                conn.commit()\n",
    "        else:\n",
    "            print url\n",
    "    except Exception as e:\n",
    "        print url, e\n",
    "        conn.commit()\n",
    "        break\n",
    "\n",
    "#sql='insert into major_table values( )'\n",
    "#major_description\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn=psycopg2.connect(database=\"resultdb\", user=\"postgres\",password=\"wszgrwhja1\", host=\"10.1.36.183\", port=\"5432\")\n",
    "cur = conn.cursor()\n",
    "from pyspider.database import connect_database\n",
    "##创建数据库\n",
    "conn.commit()\n",
    "\n",
    "##数据解析\n",
    "resultdb = connect_database(\"sqlalchemy+postgresql+resultdb://postgres:wszgrwhja1@10.1.36.183:5432/resultdb\")\n",
    "#result=resultdb.select('test6').next()\n",
    "#row_result = result['result']\n",
    "#url=row_result['wages_and_employment_content']\n",
    "#print type(url),url\n",
    "#\n",
    "for result in  resultdb.select('test7'):\n",
    "# major_title\n",
    "    try:\n",
    "        row_result = result['result']\n",
    "        url=row_result['current_url']\n",
    "        sql='select url_id from major_table where url = \\'%s\\' ' % url\n",
    "        cur.execute(sql)\n",
    "        a=cur.fetchone()\n",
    "        if a:\n",
    "##detailed_work_activities\n",
    "            url_id=a[0]\n",
    "            detailed_work_activities=row_result['detailed_work_activities']##这里和下面得改\n",
    "            if detailed_work_activities:\n",
    "                for key,value in detailed_work_activities.items():##这里和下面得改\n",
    "                        #url_id\tcontext\tquestions\tscore\texplanation\trelated_url\n",
    "                        cur.execute( \"INSERT INTO detailed_work_activities (url_id, activies,related_url) VALUES(%s,%s,%s)\",\n",
    "                          [url_id, key,value])  ##这里得改\n",
    "                        conn.commit()\n",
    "    except Exception as e:\n",
    "        print url,e\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-104-d21c0ca4cb3b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-104-d21c0ca4cb3b>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    try:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "##wages_and_employment_content\n",
    "       if a:\n",
    "            url_id=a[0]\n",
    "            wages_and_employment_content =row_result['wages_and_employment_content']##这里和下面得改\n",
    "            if  wages_and_employment_content :##这里得该\n",
    "                median_wages=wages_and_employment_content[0]#这里得修改\n",
    "                median_wages[0]=median_wages[0].encode('utf-8')\n",
    "                if '(' in median_wages[0]:\n",
    "                    median_wages[0]=median_wages[0].split('(')[1].strip(' )')\n",
    "                median_wages=':'.join(median_wages)\n",
    "                employment=wages_and_employment_content[1]#这里得修改\n",
    "                employment[0]=employment[0].encode('utf-8')\n",
    "                if '(' in employment[0]:\n",
    "                    employment[0]=employment[0].split('(')[1].strip(' )')\n",
    "                employment=':'.join(employment)\n",
    "                projected_growth=wages_and_employment_content[2]#这里得修改\n",
    "                projected_growth[0]=projected_growth[0].encode('utf-8')\n",
    "                if '(' in projected_growth[0]:\n",
    "                    projected_growth[0]=projected_growth[0].split('(')[1].strip(' )')\n",
    "                projected_growth=':'.join(projected_growth)\n",
    "                projected_job_openings=wages_and_employment_content[3]#这里得修改\n",
    "                projected_job_openings[0]=projected_job_openings[0].encode('utf-8')\n",
    "                if '(' in projected_job_openings[0]:\n",
    "                    projected_job_openings[0]=projected_job_openings[0].split('(')[1].strip(' )')\n",
    "                projected_job_openings=':'.join(projected_job_openings)\n",
    "                top_industries=wages_and_employment_content[4]#这里得修改\n",
    "                top_industries[0]=top_industries[0].encode('utf-8')\n",
    "                if '(' in top_industries[0]:\n",
    "                    top_industries[0]=top_industries[0].split('(')[1].strip(' )')                \n",
    "                top_industries=':'.join(top_industries)\n",
    "                cur.execute( \"INSERT INTO wages_and_employment_content (url_id,median_wages,employment,projected_growth,projected_job_openings,top_industries) VALUES (%s,%s,%s,%s,%s,%s)\",\n",
    "                  [url_id, median_wages,employment,projected_growth,projected_job_openings,top_industries]) ##这里得改\n",
    "                conn.commit()\n",
    "            else:\n",
    "                cur.execute( \"INSERT INTO wages_and_employment_content (url_id,median_wages,employment,projected_growth,projected_job_openings,top_industries) VALUES (%s,%s,%s,%s,%s,%s)\",\n",
    "                      [url_id,'','','','',''])  ##这里得改\n",
    "                conn.commit()\n",
    "##  if tool_content:##这里得该\n",
    "if tool_content:##这里得该\n",
    "     for key,value in tool_content.items():#\n",
    "            cur.execute( \"INSERT INTO tool_content (url_id, tool,content,related_url) VALUES(%s,%s,%s,%s)\",\n",
    "              [url_id, key,value[0],value[1]])  ##这里得改\n",
    "            conn.commit()\n",
    "else:\n",
    "    cur.execute( \"INSERT INTO tool_content (url_id, tool,content,related_url) VALUES(%s,%s,%s,%s)\",\n",
    "          [url_id, '','',''])  ##这里得改\n",
    "    conn.commit()\n",
    "##job_zone_content\n",
    "if job_zone_content:##这里得该\n",
    "     for key,value in job_zone_content.items():#\n",
    "            cur.execute( \"INSERT INTO job_zone_content (url_id, type,content) VALUES(%s,%s,%s)\",\n",
    "              [url_id, key,value])  ##这里得改\n",
    "            conn.commit()\n",
    "else:\n",
    "    cur.execute( \"INSERT INTO job_zone_content (url_id, type,content) VALUES(%s,%s,%s)\",\n",
    "      [url_id, '',''])  ##这里得改\n",
    "    conn.commit()\n",
    "##related_occupations\n",
    "if related_occupations:\n",
    "    for key in related_occupations.keys():##这里和下面得改\n",
    "            cur.execute( \"INSERT INTO related_occupations (url_id, code) VALUES(%s,%s)\",\n",
    "              [url_id, key])  ##这里得改\n",
    "            conn.commit()\n",
    "else:\n",
    "    cur.execute( \"INSERT INTO related_occupations (url_id, code) VALUES(%s,%s)\",\n",
    "      [url_id, ''])  ##这里得改\n",
    "    conn.commit()\n",
    "#work_context\n",
    "            work_context=row_result['work_context']##这里和下面得改\n",
    "            if work_context:\n",
    "                for key,value in work_context.items():##这里和下面得改\n",
    "                    questions=value[0]\n",
    "                    answer=value[1]\n",
    "                    related_url=value[2]\n",
    "                    for each in answer:\n",
    "                        #url_id\tcontext\tquestions\tscore\texplanation\trelated_url\n",
    "                        score=each[0]\n",
    "                        explanation=each[1]\n",
    "                        cur.execute(\"INSERT INTO work_context (url_id, context,questions,score,explanation,related_url) VALUES(%s,%s,%s,%s,%s,%s)\",\n",
    "                         [url_id, key,questions,score,explanation,related_url])##这里得改\n",
    "                    conn.commit()\n",
    "            else:\n",
    "                cur.execute(\"INSERT INTO work_context (url_id) VALUES(%s)\",\n",
    "                         [url_id])\n",
    "##detailed_work_activities\n",
    "            detailed_work_activities=row_result['detailed_work_activities']##这里和下面得改\n",
    "            if detailed_work_activities:\n",
    "                for key,value in detailed_work_activities.items():##这里和下面得改\n",
    "                        #url_id\tcontext\tquestions\tscore\texplanation\trelated_url\n",
    "                        cur.execute( \"INSERT INTO detailed_work_activities (url_id, activies,related_url) VALUES(%s,%s,%s)\",\n",
    "                          [url_id, key,value])  ##这里得改\n",
    "                        conn.commit()\n",
    "#education_content\n",
    "url_id=a[0]\n",
    "education_content=row_result['education_content']\n",
    "for each in education_content:\n",
    "    if each[1]=='Not available':\n",
    "        score=0\n",
    "    else:\n",
    "        score=each[1]\n",
    "    cur.execute(\"INSERT INTO education_content (url_id, degree,score) VALUES(%s,%s,%s)\",\n",
    "         [url_id, each[0],score])\n",
    "    conn.commit()\n",
    "# sample_of_job_titles\n",
    "    row_result = result['result']\n",
    "    url=row_result['current_url']\n",
    "    sql='select url_id from major_table where url = \\'%s\\' ' % url\n",
    "    cur.execute(sql)\n",
    "    a=cur.fetchone()\n",
    "    if a:\n",
    "        url_id=a[0]\n",
    "        sample_of_job_titles=row_result['sample_of_job_titles']\n",
    "        cur.execute(\"INSERT INTO sample_of_job_titles (url_id, job_title) VALUES(%s,%s)\",\n",
    "                 [url_id, sample_of_job_titles])\n",
    "        conn.commit()\n",
    "#major table \n",
    "try:\n",
    "    row_result = result['result']\n",
    "    url=row_result['current_url']\n",
    "    all_items=row_result['all_items']\n",
    "    if all_items:\n",
    "        all_items=';'.join(all_items)       \n",
    "    #all_items=\n",
    "    major_title=row_result['major_title']\n",
    "    title=major_title.split(' - ')[1].strip().encode(\"utf-8\")\n",
    "    code=major_title.split(' - ')[0].strip().encode(\"utf-8\")\n",
    "    major_description=row_result['major_description']\n",
    "#sql='insert into major_table(url, all_items ,code,title,major_description)'\n",
    "    cur.execute(\"INSERT INTO major_table(url, all_items ,code,title,major_description) VALUES(%s,%s,%s,%s,%s)\",\n",
    "                 [url, all_items ,code,title,major_description])\n",
    "    conn.commit()\n",
    "except Exception as e:\n",
    "    print url,e,type(all_items)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##error_url\n",
    "sql='''CREATE TABLE error_url \n",
    "       (id serial not null PRIMARY KEY,\n",
    "       related_url varchar(155) not null   ,\n",
    "       type varchar(50) not null   ,\n",
    "       time  timestamp  not null default current_timestamp\n",
    ")'''\n",
    "\n",
    "##related_code_content\n",
    "sql='''CREATE TABLE related_code_content \n",
    "       (id serial not null PRIMARY KEY,\n",
    "       related_url varchar(90) not null   ,\n",
    "       field  varchar(50)  null,\n",
    "       code  varchar(10)  null,\n",
    "       related_content  character varying  null\n",
    ")'''\n",
    "##related_code\n",
    "sql='''CREATE TABLE related_code \n",
    "       (id serial not null PRIMARY KEY,\n",
    "       related_url varchar(90) not null   ,\n",
    "       field  varchar(50)  null,\n",
    "       code  varchar(10)   null\n",
    ")'''\n",
    "##tasks\n",
    "\n",
    "sql='''CREATE TABLE tasks \n",
    "       (id serial not null PRIMARY KEY,\n",
    "       url_id int not null  references major_table(url_id) ,\n",
    "       task  character varying  null,\n",
    "        score int null,\n",
    "        related_url varchar(90)   null\n",
    ")'''\n",
    "cur.execute(sql)\n",
    "conn.commit()\n",
    "##work_values_content work_styles_content  work_activities skills_content knowledge_content  interests abilities tasks\n",
    "sql='''CREATE TABLE work_values_content \n",
    "       (id serial not null PRIMARY KEY,\n",
    "       url_id int not null  references major_table(url_id) ,\n",
    "       work_value  character varying not null,\n",
    "       content  character varying not null,\n",
    "        score int null,\n",
    "        related_url varchar(90)  not null\n",
    "\n",
    ")'''\n",
    "##tool_content technology_content\n",
    "sql='''CREATE TABLE tool_content \n",
    "       (id serial not null PRIMARY KEY,\n",
    "       url_id int not null  references major_table(url_id) ,\n",
    "       tool  character varying not null,\n",
    "       content  character varying not null,\n",
    "        related_url varchar(90)  not null\n",
    "\n",
    ")'''\n",
    "##job_zone_content\n",
    "sql='''CREATE TABLE job_zone_content \n",
    "       (id serial not null PRIMARY KEY,\n",
    "       url_id int not null  references major_table(url_id) ,\n",
    "       type  character varying not null,\n",
    "       content  character varying not null\n",
    ")'''\n",
    "##related_occupations\n",
    "sql='''CREATE TABLE related_occupations \n",
    "       (id serial not null PRIMARY KEY,\n",
    "       url_id int not null  references major_table(url_id) ,\n",
    "       code  varchar(10)    null \n",
    ")'''\n",
    "##work_context\n",
    "sql='''CREATE TABLE work_context \n",
    "       (id serial not null PRIMARY KEY,\n",
    "       url_id int not null  references major_table(url_id) ,\n",
    "        context character varying null,\n",
    "        questions character varying null,\n",
    "        score int null,\n",
    "        explanation character varying null,\n",
    "         related_url varchar(90)   null\n",
    ")'''\n",
    "##wages_and_employment_content\n",
    "sql='''CREATE TABLE wages_and_employment_content \n",
    "       (\n",
    "       url_id int not null  references major_table(url_id) PRIMARY KEY,\n",
    "        employment character varying null,\n",
    "        median_wages character varying null,\n",
    "        projected_growth  character varying null,\n",
    "        projected_job_openings character varying null,\n",
    "        top_industries character varying null\n",
    ")'''\n",
    "##detailed_work_activities\n",
    "sql='''CREATE TABLE detailed_work_activities \n",
    "       (id serial not null PRIMARY KEY,\n",
    "       url_id int not null references major_table(url_id),\n",
    "       activies character varying  null ,\n",
    "       related_url varchar(90)   null\n",
    ")'''\n",
    "##education_content\n",
    "sql='''CREATE TABLE education_content \n",
    "       (id serial not null PRIMARY KEY,\n",
    "       url_id int not null references major_table(url_id),\n",
    "       degree character varying  null ,\n",
    "       score  int null\n",
    ")'''\n",
    "##sample_of_job_titles \n",
    "sql='''CREATE TABLE sample_of_job_titles \n",
    "       (id serial not null PRIMARY KEY,\n",
    "       url_id int not null references major_table(url_id),\n",
    "       job_title  character varying  null \n",
    ")'''\n",
    "##major_table\n",
    "sql='''CREATE TABLE major_table\n",
    "       (url_id serial not null PRIMARY KEY,\n",
    "       url varchar(49) ,\n",
    "       code  character varying not null ,\n",
    "       title character varying not null,\n",
    "       all_items  character varying null ,\n",
    "       major_description  character varying null)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
